## ğŸ§  Ollama-Pinecone-RAG

A lightweight **Retrieval-Augmented Generation (RAG)** pipeline that indexes PDF documents, stores their embeddings in **Pinecone**, and answers user queries using **Ollamaâ€™s local LLMs (Gemma)**.

---

### ğŸš€ Features

* ğŸ“„ Load and split PDFs using **LangChain**
* ğŸ” Generate embeddings with **Ollama EmbeddingGemma**
* ğŸ§© Store and query embeddings via **Pinecone Vector Database**
* ğŸ’¬ Generate context-aware answers using **Gemma-3 4B**
* âš™ï¸ Simple, local-first, and easily extensible

---

### ğŸ› ï¸ Installation

```bash
# Clone this repo
git clone https://github.com/<your-username>/ollama-pinecone-rag.git
cd ollama-pinecone-rag

# Install dependencies
pip install -r requirements.txt
```

---

### âš™ï¸ Configuration

Edit the following variables in `main.py` before running:

```python
PDF_PATH = "path/to/your/pdf/file.pdf"
INDEX_NAME = "your-pinecone-index-name"
PINECONE_API_KEY = "your-pinecone-api-key"
```

For security, you can store your API key in an environment variable:

```bash
export PINECONE_API_KEY="your-pinecone-api-key"
```

---

### â–¶ï¸ Usage

```bash
python main.py
```

This will:

1. Load your PDF
2. Chunk it into text segments
3. Generate and upload embeddings to Pinecone
4. Answer your test query:

   > "Summarize the main findings of the PDF."

---

### ğŸ“¦ Project Structure

```
ollama-pinecone-rag/
â”‚
â”œâ”€â”€ main.py             # Main RAG pipeline script
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Project documentation
â””â”€â”€ .gitignore          # Git ignore rules
```

---

### ğŸ§© Next Steps

* Add a Flask or Streamlit UI for interactive querying
* Integrate multi-document retrieval
* Add reranking or summarization before context generation

---
